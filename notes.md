This repository processes datasets typically used in our work in environment and biodiversity into cloud-optimized formats for visualization (PMTiles and COGS) and h3geo-indexed Parquet for computation with duckdb.  

Your job is to carefully follow the instructions and use the utilities here to generate and run the jobs to process additional datasets.  


These tasks are computationally intensive.  To scale to global datasets, we rely on deploying the process as a series of jobs on our large k8s cluster, NRP.  All data is pushed to the S3 Ceph cluster on NRP. 

A python helper package, cng_datasets, creates the kubernetes job yaml and provides the helper routines to perform the processing required for both raster and vector inputs. 


Once processed datasets are made, documentation of each dataset into both markdown and STAC catalog metadata uses an LLM-assisted workflow as described in DATASET_DOCUMENTATION_WORKFLOW.md

This has been an evolving effort, and some early products were processed in less automated ways that may give rise to inconsistencies and need re-processing.  See todo.md


Important notes on the Kubernetes cluster:  

- Our namespace is 'biodiversity'.  All jobs should be run in this namespace, you do not have access to other namespaces.
- Secrets required by the jobs (as generated by cng-datasets utility) should already be present in the namespace.
- The NRP k8s cluster is a shared academic resource. cng-datasets routine is careful to include important affinities that target nodes without GPUs, and importantly, allow for prememption. 
- The intensive 'hex' job utilizes up to 200 'completions'. Do not run more than 200 completions in a single job as this can overwhelm the k8s cluster's central etcd system. If necessary, an experimental mechanism called Armada can be used to handle arbitirarily large numbers of completions.  

Notes on S3 buckets: 

- All the final outputs are on the Ceph S3 system for the k8s cluster.