{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8558b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/boettiger-lab/cng-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea39c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ibis\n",
    "from ibis import _\n",
    "from cng.utils import *\n",
    "from cng.h3 import * \n",
    "import os\n",
    "con = ibis.duckdb.connect(\"local.db\", extensions = [\"spatial\", \"h3\"])\n",
    "install_h3()\n",
    "\n",
    "\n",
    "# Must used scoped secrets with different names for the different endpoints\n",
    "set_secrets(con, name = \"minio\") # read/write using AWS env var credentials\n",
    "set_secrets(con, \"\", \"\", endpoint = \"s3.amazonaws.com\", region=\"us-west-2\", name = \"source\", bucket = \"us-west-2.opendata.source.coop\")\n",
    "\n",
    "def geom_to_cell(df, zoom=8, keep_cols=None):\n",
    "    con = df.get_backend()\n",
    "    \n",
    "    # Default to keeping all columns except geom if not specified\n",
    "    if keep_cols is None:\n",
    "        keep_cols = [col for col in df.columns if col != 'geom']\n",
    "    \n",
    "    # Build column list for SELECT statements\n",
    "    col_list = ', '.join(keep_cols)\n",
    "    \n",
    "    # all types must be multi-polygons\n",
    "    cases = ibis.cases(\n",
    "        (df.geom.geometry_type() == 'POLYGON', ST_Multi(df.geom)),\n",
    "        else_=df.geom,\n",
    "    )\n",
    "    \n",
    "    df = df.mutate(geom=cases)\n",
    "    sql = ibis.to_sql(df)\n",
    "    \n",
    "    expr = f'''\n",
    "        WITH t1 AS (\n",
    "            SELECT {col_list}, UNNEST(ST_Dump(ST_GeomFromWKB(geom))).geom AS geom \n",
    "            FROM ({sql})\n",
    "        ) \n",
    "        SELECT *, h3_polygon_wkt_to_cells_string(geom, {zoom}) AS h3id FROM t1\n",
    "    '''\n",
    "\n",
    "    out = con.sql(expr)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46534973",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"s3://us-west-2.opendata.source.coop/giswqs/nwi/wetlands/**\"\n",
    "SOURCE = \"s3://public-nwi/aws/us-west-2.opendata.source.coop/giswqs/nwi/wetlands/**\"\n",
    "\n",
    "nwi =(con\n",
    "    .read_parquet(SOURCE, filename = True)\n",
    "    .select('geometry', 'ATTRIBUTE', 'WETLAND_TYPE', 'filename')\n",
    "    .rename(geom = \"geometry\")\n",
    "    .mutate(state_code=_.filename.re_extract(r\"([A-Z]{2})_Wetlands.parquet\", 1))\n",
    "    .mutate(geom =  _.geom.convert('EPSG:5070','EPSG:4326'))\n",
    "    .drop('filename')\n",
    ")\n",
    "\n",
    "\n",
    "x = nwi.head().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db090053",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_LIMIT='20GB'\n",
    "CHUNK_SIZE = 10000\n",
    "con.raw_sql(f\"SET memory_limit='{MEMORY_LIMIT}';\")\n",
    "\n",
    "OUTPUT_PATH=\"s3://public-wetlands/nwi/\"\n",
    "\n",
    "\n",
    "table = nwi\n",
    "# Read parquet file\n",
    "\n",
    "# Get total row count and calculate chunks\n",
    "total_rows = table.count().execute()\n",
    "num_chunks = (total_rows + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(f\"Chunk size: {CHUNK_SIZE:,}\")\n",
    "print(f\"Number of chunks: {num_chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunk_id = 0\n",
    "offset = chunk_id * CHUNK_SIZE\n",
    "print(f\"\\nProcessing chunk {chunk_id + 1}/{num_chunks} (rows {offset:,} to {min(offset + CHUNK_SIZE, total_rows):,})\")\n",
    "\n",
    "chunk = table.limit(CHUNK_SIZE, offset=offset)\n",
    "result = geom_to_cell(chunk, zoom=8).mutate(h8 = _.h3id.unnest()).drop('h3id')\n",
    "\n",
    "result.to_parquet(\"test_10.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.read_parquet(\"test_10.parquet\").head().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c28680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process each chunk\n",
    "for chunk_id in range(num_chunks):\n",
    "    offset = chunk_id * CHUNK_SIZE\n",
    "    print(f\"\\nProcessing chunk {chunk_id + 1}/{num_chunks} (rows {offset:,} to {min(offset + CHUNK_SIZE, total_rows):,})\")\n",
    "    \n",
    "    # Get chunk with row filtering\n",
    "    chunk = table.limit(CHUNK_SIZE, offset=offset)\n",
    "    result = geom_to_cell(chunk, zoom=8).mutate(h8 = _.h3id.unnest())\n",
    "    \n",
    "    # Write to parquet\n",
    "    output_file = f\"{OUTPUT_PATH}chunks/chunk_{chunk_id:04d}.parquet\"\n",
    "    result.to_parquet(output_file)\n",
    "    \n",
    "    print(f\"  ✓ Chunk {chunk_id} written\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35677081",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n✅ All chunks processed!\")\n",
    "\n",
    "# Combine all chunks\n",
    "print(\"\\nCombining chunks...\")\n",
    "combined = con.read_parquet(f'{OUTPUT_PATH}/chunks/chunk_*.parquet')\n",
    "combined.to_parquet(f'{OUTPUT_PATH}/hex/combined_results.parquet')\n",
    "print(\"✅ Combined file created!\")\n",
    "\n",
    "con.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
